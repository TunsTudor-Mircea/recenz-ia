{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df28c37a",
   "metadata": {},
   "source": [
    "# XGBoost Preprocessing Pipeline Visualization\n",
    "\n",
    "This notebook demonstrates how Romanian reviews are preprocessed at each step of the XGBoost pipeline.\n",
    "\n",
    "We'll see the transformation from raw text through:\n",
    "1. **Original Text** - Raw review as scraped\n",
    "2. **After Cleaning** - Lowercase, special characters handled\n",
    "3. **After Tokenization** - Split into words, filtered by length\n",
    "4. **After Lemmatization** - Words reduced to base forms, stopwords removed\n",
    "5. **After Stemming** - Further reduction to word stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e58668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Import preprocessing components\n",
    "from preprocessing.cleaner import TextCleaner\n",
    "from preprocessing.tokenizer import WhitespaceTokenizer\n",
    "from preprocessing.lemmatizer import RomanianLemmatizer\n",
    "from preprocessing.stemmer import RomanianStemmer\n",
    "from preprocessing.pipeline import PreprocessingPipeline\n",
    "from utils.config import load_config\n",
    "\n",
    "print(\"Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff764dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LaRoSeDa dataset...\n",
      "Loaded 12000 reviews\n",
      "\n",
      "Columns: ['index', 'title', 'content', 'starRating']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>starRating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11262</td>\n",
       "      <td>Foarte slab</td>\n",
       "      <td>ca aspect este foarte frumoasa dar cine vrea s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3890</td>\n",
       "      <td>Foarte multumit</td>\n",
       "      <td>se incarca repede si tine 3 incarcari complete...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9413</td>\n",
       "      <td>»öeapa de zile mari!!!</td>\n",
       "      <td>in primul rand nu este de stica dupa cum spune...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9350</td>\n",
       "      <td>Nu merita cumparate</td>\n",
       "      <td>nu merita cumparate... sunt create cu limitare...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7126</td>\n",
       "      <td>Recomand</td>\n",
       "      <td>un ceas excelent. face cam tot ce ai nevoie ca...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                  title  \\\n",
       "0  11262            Foarte slab   \n",
       "1   3890        Foarte multumit   \n",
       "2   9413  »öeapa de zile mari!!!   \n",
       "3   9350    Nu merita cumparate   \n",
       "4   7126               Recomand   \n",
       "\n",
       "                                             content  starRating  \n",
       "0  ca aspect este foarte frumoasa dar cine vrea s...           1  \n",
       "1  se incarca repede si tine 3 incarcari complete...           5  \n",
       "2  in primul rand nu este de stica dupa cum spune...           1  \n",
       "3  nu merita cumparate... sunt create cu limitare...           1  \n",
       "4  un ceas excelent. face cam tot ce ai nevoie ca...           5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a sample of reviews from LaRoSeDa dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"Loading LaRoSeDa dataset...\")\n",
    "train_file = hf_hub_download(\n",
    "    repo_id=\"universityofbucharest/laroseda\",\n",
    "    filename=\"laroseda/train/0000.parquet\",\n",
    "    repo_type=\"dataset\",\n",
    "    revision=\"refs/convert/parquet\"\n",
    ")\n",
    "\n",
    "df = pd.read_parquet(train_file)\n",
    "print(f\"Loaded {len(df)} reviews\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2fb0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected samples:\n",
      "\n",
      "1. NEGATIVE (1 stars)\n",
      "   Length: 293 chars\n",
      "   Preview: »öeapa de zile mari!!! in primul rand nu este de stica dupa cum spune producatorul, este una de plast...\n",
      "\n",
      "2. POSITIVE (5 stars)\n",
      "   Length: 163 chars\n",
      "   Preview: Recomand am acest produs de aproape jumatate de an timp in care nu am avut probleme cu el. s-a fixat...\n"
     ]
    }
   ],
   "source": [
    "# Select a few diverse examples\n",
    "# Let's get both positive and negative reviews with varying lengths\n",
    "\n",
    "# Get one negative (1-2 stars) and one positive (4-5 stars)\n",
    "negative_sample = df[df['starRating'].isin([1, 2])].iloc[1]\n",
    "positive_sample = df[df['starRating'].isin([4, 5])].iloc[3]\n",
    "\n",
    "# Combine title and content as done in training\n",
    "sample_reviews = [\n",
    "    {\n",
    "        'sentiment': 'NEGATIVE',\n",
    "        'stars': negative_sample['starRating'],\n",
    "        'text': f\"{negative_sample['title']} {negative_sample['content']}\"\n",
    "    },\n",
    "    {\n",
    "        'sentiment': 'POSITIVE',\n",
    "        'stars': positive_sample['starRating'],\n",
    "        'text': f\"{positive_sample['title']} {positive_sample['content']}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Selected samples:\")\n",
    "for i, review in enumerate(sample_reviews, 1):\n",
    "    print(f\"\\n{i}. {review['sentiment']} ({review['stars']} stars)\")\n",
    "    print(f\"   Length: {len(review['text'])} chars\")\n",
    "    print(f\"   Preview: {review['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0061839",
   "metadata": {},
   "source": [
    "## Step-by-Step Preprocessing\n",
    "\n",
    "Now let's see how each component transforms the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18c51d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing configuration:\n",
      "{'preprocessing': {'language': 'romanian', 'lowercase': True, 'remove_stopwords': True, 'lemmatize': True, 'stem': True, 'min_token_length': 2, 'max_token_length': 50}}\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessing config (same as used in training)\n",
    "preprocessing_config = load_config('../configs/preprocessing_config.yaml')\n",
    "print(\"Preprocessing configuration:\")\n",
    "print(preprocessing_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8f5053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All preprocessing components initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize each component separately to see individual effects\n",
    "\n",
    "# Extract the preprocessing section from config\n",
    "config = preprocessing_config['preprocessing']\n",
    "\n",
    "cleaner = TextCleaner(\n",
    "    lowercase=config['lowercase']\n",
    ")\n",
    "\n",
    "tokenizer = WhitespaceTokenizer(\n",
    "    min_token_length=config['min_token_length'],\n",
    "    max_token_length=config['max_token_length']\n",
    ")\n",
    "\n",
    "lemmatizer = RomanianLemmatizer(\n",
    "    remove_stopwords=config['remove_stopwords']\n",
    ")\n",
    "\n",
    "stemmer = RomanianStemmer()\n",
    "\n",
    "print(\"All preprocessing components initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73939fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_preprocessing(text, sentiment_label):\n",
    "    \"\"\"\n",
    "    Show each preprocessing step for a single review.\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(f\"SENTIMENT: {sentiment_label}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Step 0: Original\n",
    "    print(f\"\\nüìù STEP 0: ORIGINAL TEXT\")\n",
    "    print(f\"-\" * 100)\n",
    "    print(text[:500])  # Show first 500 chars\n",
    "    if len(text) > 500:\n",
    "        print(f\"... (total {len(text)} chars)\")\n",
    "    \n",
    "    # Step 1: Cleaning\n",
    "    cleaned = cleaner.clean(text)\n",
    "    print(f\"\\nüßπ STEP 1: AFTER CLEANING (lowercase, normalize)\")\n",
    "    print(f\"-\" * 100)\n",
    "    print(cleaned[:500])\n",
    "    if len(cleaned) > 500:\n",
    "        print(f\"... (total {len(cleaned)} chars)\")\n",
    "    \n",
    "    # Step 2: Tokenization\n",
    "    tokens = tokenizer.tokenize(cleaned)\n",
    "    print(f\"\\n‚úÇÔ∏è STEP 2: AFTER TOKENIZATION (split, filter length)\")\n",
    "    print(f\"-\" * 100)\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(f\"Tokens: {' | '.join(tokens[:30])}\")\n",
    "    if len(tokens) > 30:\n",
    "        print(f\"... and {len(tokens) - 30} more tokens\")\n",
    "    \n",
    "    # Step 3: Lemmatization (also removes stopwords)\n",
    "    lemmatized = lemmatizer.lemmatize(tokens)\n",
    "    print(f\"\\nüî§ STEP 3: AFTER LEMMATIZATION (base forms + stopword removal)\")\n",
    "    print(f\"-\" * 100)\n",
    "    print(f\"Token count: {len(lemmatized)} (removed {len(tokens) - len(lemmatized)} stopwords)\")\n",
    "    print(f\"Tokens: {' | '.join(lemmatized[:30])}\")\n",
    "    if len(lemmatized) > 30:\n",
    "        print(f\"... and {len(lemmatized) - 30} more tokens\")\n",
    "    \n",
    "    # Step 4: Stemming\n",
    "    stemmed = stemmer.stem(lemmatized)\n",
    "    print(f\"\\nüå± STEP 4: AFTER STEMMING (further reduction)\")\n",
    "    print(f\"-\" * 100)\n",
    "    print(f\"Token count: {len(stemmed)}\")\n",
    "    print(f\"Tokens: {' | '.join(stemmed[:30])}\")\n",
    "    if len(stemmed) > 30:\n",
    "        print(f\"... and {len(stemmed) - 30} more tokens\")\n",
    "    \n",
    "    # Final output\n",
    "    final_text = ' '.join(stemmed)\n",
    "    print(f\"\\n‚ú® FINAL PREPROCESSED TEXT\")\n",
    "    print(f\"-\" * 100)\n",
    "    print(final_text[:500])\n",
    "    if len(final_text) > 500:\n",
    "        print(f\"... (total {len(final_text)} chars)\")\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"   Original length: {len(text)} chars, ~{len(text.split())} words\")\n",
    "    print(f\"   After tokenization: {len(tokens)} tokens\")\n",
    "    print(f\"   After lemmatization: {len(lemmatized)} tokens ({100*(len(tokens)-len(lemmatized))/len(tokens):.1f}% removed)\")\n",
    "    print(f\"   After stemming: {len(stemmed)} tokens\")\n",
    "    print(f\"   Final text length: {len(final_text)} chars\")\n",
    "    print(f\"   Reduction: {100*(len(text)-len(final_text))/len(text):.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f140740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SENTIMENT: NEGATIVE (1 stars)\n",
      "====================================================================================================\n",
      "\n",
      "üìù STEP 0: ORIGINAL TEXT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "»öeapa de zile mari!!! in primul rand nu este de stica dupa cum spune producatorul, este una de plastic de nici 10 lei   calitatea este 0 a mea a venit gata zgariata plus de asta nu se lipeste pe ecran sub nici o forma desi am urmat atent instructiunile   daca nu aveti pe ce da bani comandati!\n",
      "\n",
      "üßπ STEP 1: AFTER CLEANING (lowercase, normalize)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "»õeapa de zile mari!!! in primul rand nu este de stica dupa cum spune producatorul este una de plastic de nici 10 lei calitatea este 0 a mea a venit gata zgariata plus de asta nu se lipeste pe ecran sub nici o forma desi am urmat atent instructiunile daca nu aveti pe ce da bani comandati!\n",
      "\n",
      "‚úÇÔ∏è STEP 2: AFTER TOKENIZATION (split, filter length)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Token count: 53\n",
      "Tokens: »õeapa | de | zile | mari!!! | in | primul | rand | nu | este | de | stica | dupa | cum | spune | producatorul | este | una | de | plastic | de | nici | 10 | lei | calitatea | este | mea | venit | gata | zgariata | plus\n",
      "... and 23 more tokens\n",
      "\n",
      "üî§ STEP 3: AFTER LEMMATIZATION (base forms + stopword removal)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Token count: 30 (removed 23 stopwords)\n",
      "Tokens: »õeapƒÉ | zi | marie | rand | nu | sticƒÉ | spune | producator | plastic | nici | 10 | leu | calitate | veni | gata | zgariatƒÉ | nu | sine | lipeste | ecran | sub | nici | formƒÉ | urma | atent | instructiune | nu | avet | ban | comandati!\n",
      "\n",
      "üå± STEP 4: AFTER STEMMING (further reduction)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Token count: 30\n",
      "Tokens: »õeap | zi | mar | rand | nu | stic | spun | produc | plastic | nic | 10 | leu | calitat | ven | gat | zgariat | nu | sin | lipest | ecran | sub | nic | form | urma | atent | instructiun | nu | avet | ban | comandati!\n",
      "\n",
      "‚ú® FINAL PREPROCESSED TEXT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "»õeap zi mar rand nu stic spun produc plastic nic 10 leu calitat ven gat zgariat nu sin lipest ecran sub nic form urma atent instructiun nu avet ban comandati!\n",
      "\n",
      "üìä SUMMARY:\n",
      "   Original length: 293 chars, ~57 words\n",
      "   After tokenization: 53 tokens\n",
      "   After lemmatization: 30 tokens (43.4% removed)\n",
      "   After stemming: 30 tokens\n",
      "   Final text length: 158 chars\n",
      "   Reduction: 46.1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize preprocessing for NEGATIVE review\n",
    "negative_preprocessed = visualize_preprocessing(\n",
    "    sample_reviews[0]['text'],\n",
    "    f\"{sample_reviews[0]['sentiment']} ({sample_reviews[0]['stars']} stars)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf0fe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SENTIMENT: POSITIVE (5 stars)\n",
      "====================================================================================================\n",
      "\n",
      "üìù STEP 0: ORIGINAL TEXT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Recomand am acest produs de aproape jumatate de an timp in care nu am avut probleme cu el. s-a fixat din prima, telefoanele stau bine si e foarte usor de manevrat.\n",
      "\n",
      "üßπ STEP 1: AFTER CLEANING (lowercase, normalize)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "recomand am acest produs de aproape jumatate de an timp in care nu am avut probleme cu el. s a fixat din prima telefoanele stau bine si e foarte usor de manevrat.\n",
      "\n",
      "‚úÇÔ∏è STEP 2: AFTER TOKENIZATION (split, filter length)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Token count: 29\n",
      "Tokens: recomand | am | acest | produs | de | aproape | jumatate | de | an | timp | in | care | nu | am | avut | probleme | cu | el. | fixat | din | prima | telefoanele | stau | bine | si | foarte | usor | de | manevrat.\n",
      "\n",
      "üî§ STEP 3: AFTER LEMMATIZATION (base forms + stopword removal)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Token count: 16 (removed 13 stopwords)\n",
      "Tokens: recomanda | produs | aproape | jumatate | an | nu | problemƒÉ | elebprn | fixa | telefon | sta | bine | si | foarte | usor | manevrat\n",
      "\n",
      "üå± STEP 4: AFTER STEMMING (further reduction)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Token count: 16\n",
      "Tokens: recomand | produs | aproap | jumat | an | nu | problem | elebprn | fix | telefon | sta | bin | si | foart | usor | manevr\n",
      "\n",
      "‚ú® FINAL PREPROCESSED TEXT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "recomand produs aproap jumat an nu problem elebprn fix telefon sta bin si foart usor manevr\n",
      "\n",
      "üìä SUMMARY:\n",
      "   Original length: 163 chars, ~31 words\n",
      "   After tokenization: 29 tokens\n",
      "   After lemmatization: 16 tokens (44.8% removed)\n",
      "   After stemming: 16 tokens\n",
      "   Final text length: 91 chars\n",
      "   Reduction: 44.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize preprocessing for POSITIVE review\n",
    "positive_preprocessed = visualize_preprocessing(\n",
    "    sample_reviews[1]['text'],\n",
    "    f\"{sample_reviews[1]['sentiment']} ({sample_reviews[1]['stars']} stars)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
