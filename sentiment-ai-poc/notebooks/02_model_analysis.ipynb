{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Analysis\n",
    "\n",
    "This notebook analyzes the performance of trained sentiment analysis models.\n",
    "\n",
    "**Contents:**\n",
    "1. Load trained model and results\n",
    "2. Performance metrics visualization\n",
    "3. Confusion matrix analysis\n",
    "4. Error analysis\n",
    "5. Feature importance (if available)\n",
    "6. Prediction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "from models.xgboost_model import XGBoostModel\n",
    "from features.selector import FeatureSelector\n",
    "from preprocessing.pipeline import PreprocessingPipeline\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model\n",
    "\n",
    "**Update the experiment name below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Specify your experiment directory\n",
    "EXPERIMENT_NAME = 'xgb_optimized'  # Change this to your experiment name\n",
    "experiment_dir = Path.cwd().parent / 'results' / 'experiments' / EXPERIMENT_NAME\n",
    "\n",
    "print(f\"Loading model from: {experiment_dir}\")\n",
    "print(f\"Experiment exists: {experiment_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model components\n",
    "preprocessor = PreprocessingPipeline.load(experiment_dir / 'preprocessor.joblib')\n",
    "selector = FeatureSelector.load(experiment_dir / 'feature_selector.joblib')\n",
    "model = XGBoostModel.load(experiment_dir / 'xgboost_model.joblib')\n",
    "\n",
    "print(\"✓ Preprocessor loaded\")\n",
    "print(\"✓ Feature selector loaded\")\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load metrics\n",
    "with open(experiment_dir / 'metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision_macro']:.4f}\")\n",
    "print(f\"Recall:    {metrics['recall_macro']:.4f}\")\n",
    "print(f\"F1 Score:  {metrics['f1_macro']:.4f}\")\n",
    "\n",
    "if 'roc_auc_ovr' in metrics:\n",
    "    print(f\"ROC AUC:   {metrics['roc_auc_ovr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create metrics comparison chart\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "metric_values = [\n",
    "    metrics['accuracy'],\n",
    "    metrics['precision_macro'],\n",
    "    metrics['recall_macro'],\n",
    "    metrics['f1_macro']\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(metric_names, metric_values, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'], \n",
    "              edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Per-class metrics\n",
    "class_names = metrics.get('class_names', ['class_0', 'class_1'])\n",
    "\n",
    "per_class_metrics = pd.DataFrame({\n",
    "    'Precision': [metrics['per_class_precision'][c] for c in class_names],\n",
    "    'Recall': [metrics['per_class_recall'][c] for c in class_names],\n",
    "    'F1-Score': [metrics['per_class_f1'][c] for c in class_names]\n",
    "}, index=class_names)\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(per_class_metrics)\n",
    "\n",
    "# Visualize per-class metrics\n",
    "ax = per_class_metrics.plot(kind='bar', figsize=(10, 6), rot=0, \n",
    "                             color=['#3498db', '#2ecc71', '#f39c12'],\n",
    "                             edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.legend(title='Metric', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display confusion matrix image\n",
    "cm_path = experiment_dir / 'plots' / 'confusion_matrix.png'\n",
    "\n",
    "if cm_path.exists():\n",
    "    img = Image.open(cm_path)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"Confusion matrix plot not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze confusion matrix\n",
    "cm = np.array(metrics['confusion_matrix'])\n",
    "\n",
    "print(\"\\nConfusion Matrix Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    true_positives = cm[i, i]\n",
    "    false_positives = cm[:, i].sum() - cm[i, i]\n",
    "    false_negatives = cm[i, :].sum() - cm[i, i]\n",
    "    true_negatives = cm.sum() - (true_positives + false_positives + false_negatives)\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  True Positives:  {true_positives:5d}\")\n",
    "    print(f\"  False Positives: {false_positives:5d}\")\n",
    "    print(f\"  False Negatives: {false_negatives:5d}\")\n",
    "    print(f\"  True Negatives:  {true_negatives:5d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display ROC curves\n",
    "roc_path = experiment_dir / 'plots' / 'roc_curves.png'\n",
    "\n",
    "if roc_path.exists():\n",
    "    img = Image.open(roc_path)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('ROC Curves', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC curves plot not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display Precision-Recall curves\n",
    "pr_path = experiment_dir / 'plots' / 'precision_recall.png'\n",
    "\n",
    "if pr_path.exists():\n",
    "    img = Image.open(pr_path)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Precision-Recall Curves', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Precision-Recall curves plot not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load hyperparameters\n",
    "hp_path = experiment_dir / 'best_hyperparameters.json'\n",
    "\n",
    "if hp_path.exists():\n",
    "    with open(hp_path, 'r') as f:\n",
    "        hyperparams = json.load(f)\n",
    "    \n",
    "    print(\"\\nModel Hyperparameters:\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in hyperparams.items():\n",
    "        print(f\"{key:20s}: {value}\")\n",
    "else:\n",
    "    print(\"Hyperparameters file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for Optuna study results\n",
    "optuna_path = experiment_dir / 'optuna_study.json'\n",
    "\n",
    "if optuna_path.exists():\n",
    "    with open(optuna_path, 'r') as f:\n",
    "        optuna_results = json.load(f)\n",
    "    \n",
    "    print(\"\\nOptuna Optimization Results:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of trials: {optuna_results['n_trials']}\")\n",
    "    print(f\"Best validation accuracy: {optuna_results['best_value']:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for key, value in optuna_results['best_params'].items():\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "else:\n",
    "    print(\"\\nNo Optuna optimization results found (standard training used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test on custom reviews\n",
    "test_reviews = [\n",
    "    \"Produsul este excelent! Foarte mulțumit de calitate. Recomand cu încredere!\",\n",
    "    \"Dezamăgitor. Calitate proastă și nu funcționează cum trebuie. Nu recomand.\",\n",
    "    \"Produs bun, raport calitate-preț acceptabil. Livrare rapidă.\",\n",
    "    \"Foarte rău! Am returnat produsul. Îmi pare rău că l-am cumpărat.\",\n",
    "    \"Perfect! Exact ce căutam. Mulțumesc!\"\n",
    "]\n",
    "\n",
    "print(\"\\nPrediction Examples:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Preprocess\n",
    "processed = preprocessor.preprocess_batch(test_reviews)\n",
    "\n",
    "# Extract features\n",
    "features = selector.transform(processed)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(features)\n",
    "probabilities = model.predict_proba(features)\n",
    "\n",
    "# Display results\n",
    "for i, review in enumerate(test_reviews):\n",
    "    pred_class = class_names[predictions[i]]\n",
    "    confidence = probabilities[i][predictions[i]]\n",
    "    \n",
    "    print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"Text: {review}\")\n",
    "    print(f\"Prediction: {pred_class}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")\n",
    "    print(f\"Probabilities: {dict(zip(class_names, probabilities[i]))}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_sentiment(text):\n",
    "    \"\"\"Predict sentiment for a given text.\"\"\"\n",
    "    # Preprocess\n",
    "    processed = preprocessor.preprocess_batch([text])\n",
    "    \n",
    "    # Extract features\n",
    "    features = selector.transform(processed)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features)[0]\n",
    "    probabilities = model.predict_proba(features)[0]\n",
    "    \n",
    "    # Display results\n",
    "    pred_class = class_names[prediction]\n",
    "    confidence = probabilities[prediction]\n",
    "    \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"\\nPrediction: {pred_class}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")\n",
    "    print(f\"\\nProbabilities:\")\n",
    "    for cls, prob in zip(class_names, probabilities):\n",
    "        print(f\"  {cls}: {prob:.2%}\")\n",
    "    \n",
    "    return pred_class, confidence\n",
    "\n",
    "# Try it out!\n",
    "# predict_sentiment(\"Your Romanian review text here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook analyzed the trained sentiment analysis model:\n",
    "\n",
    "1. **Overall Performance**: Loaded and visualized key metrics\n",
    "2. **Class-wise Analysis**: Per-class precision, recall, and F1 scores\n",
    "3. **Confusion Matrix**: Detailed breakdown of predictions\n",
    "4. **ROC & PR Curves**: Model discrimination ability\n",
    "5. **Hyperparameters**: Model configuration and optimization results\n",
    "6. **Predictions**: Examples on custom Romanian reviews\n",
    "\n",
    "The model shows strong performance on binary sentiment classification for Romanian e-commerce reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
