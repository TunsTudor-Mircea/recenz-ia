# Recenz-IA

AI-powered sentiment analysis system for Romanian e-commerce product reviews.

## Overview

High-performance sentiment analysis for Romanian language reviews with two powerful models:
- **RoBERT**: 94-95% accuracy (Romanian BERT transformer)
- **XGBoost**: 91.6% accuracy (traditional ML with LF-MICF + IGWO)

## Quick Start

```bash
cd sentiment-ai-poc

# Setup
python -m venv venv
venv\Scripts\activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# Train RoBERT (recommended - best accuracy)
python scripts/train_robert.py --experiment-name robert_experiment

# Train XGBoost (fast alternative)
python scripts/train_xgboost_optimized.py --experiment-name xgb_experiment --n-trials 50

# Make predictions
python scripts/predict_robert.py --model results/experiments/robert_*/robert_model --text "Produsul este excelent!"
```

**Full documentation:** See [sentiment-ai-poc/README.md](sentiment-ai-poc/README.md)

## Features

- **Two Model Options**: Choose between accuracy (RoBERT 94-95%) or speed (XGBoost 91.6%)
- **Romanian Language**: Specialized models for Romanian text understanding
- **Production Ready**: Full model serialization, metrics tracking, and visualization
- **Fast Inference**: <1ms (XGBoost) or ~20ms (RoBERT) per review
- **Comprehensive Analysis**: Jupyter notebooks for model comparison and evaluation
- **GPU Support**: Optional GPU acceleration for faster training

## Technology

### RoBERT Model
- **Architecture**: Romanian BERT transformer (`dumitrescustefan/bert-base-romanian-cased-v1`)
- **Preprocessing**: Minimal cleaning (URL/HTML removal)
- **Training**: Fine-tuning with Hugging Face Transformers
- **Accuracy**: 94-95% (expected)

### XGBoost Model
- **Feature Engineering**: LF-MICF extraction + IGWO selection
- **Classification**: XGBoost with Optuna optimization
- **NLP**: Stanza lemmatization (Romanian)
- **Accuracy**: 91.6%

### Shared Components
- **Dataset**: LaRoSeDa (15k Romanian e-commerce reviews)
- **Analysis**: Jupyter notebooks with matplotlib/seaborn
- **Evaluation**: Comprehensive metrics and visualizations

## Project Structure

```
recenz-ia/
â””â”€â”€ sentiment-ai-poc/          # Main project
    â”œâ”€â”€ configs/               # YAML configurations
    â”‚   â”œâ”€â”€ robert_config.yaml        # RoBERT hyperparameters
    â”‚   â””â”€â”€ preprocessing_config.yaml # XGBoost preprocessing
    â”œâ”€â”€ notebooks/             # Jupyter analysis notebooks
    â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
    â”‚   â”œâ”€â”€ 02_model_analysis.ipynb
    â”‚   â””â”€â”€ 03_robert_vs_xgboost.ipynb  # Model comparison
    â”œâ”€â”€ scripts/               # Training & inference scripts
    â”‚   â”œâ”€â”€ train_robert.py              # RoBERT training
    â”‚   â”œâ”€â”€ predict_robert.py            # RoBERT inference
    â”‚   â”œâ”€â”€ train_xgboost.py             # XGBoost training
    â”‚   â””â”€â”€ train_xgboost_optimized.py   # XGBoost with Optuna
    â”œâ”€â”€ src/                   # Source modules
    â”‚   â”œâ”€â”€ preprocessing/     # Text processing
    â”‚   â”‚   â”œâ”€â”€ basic_cleaner.py        # Minimal cleaning (RoBERT)
    â”‚   â”‚   â”œâ”€â”€ lemmatizer.py           # Romanian lemmatization
    â”‚   â”‚   â””â”€â”€ pipeline.py             # Full pipeline (XGBoost)
    â”‚   â”œâ”€â”€ features/          # Feature engineering
    â”‚   â”‚   â”œâ”€â”€ lf_micf.py              # LF-MICF extraction
    â”‚   â”‚   â””â”€â”€ igwo.py                 # IGWO feature selection
    â”‚   â”œâ”€â”€ models/            # Models
    â”‚   â”‚   â”œâ”€â”€ robert_model.py         # RoBERT wrapper
    â”‚   â”‚   â””â”€â”€ xgboost_model.py        # XGBoost classifier
    â”‚   â”œâ”€â”€ evaluation/        # Metrics & visualization
    â”‚   â””â”€â”€ utils/             # Utilities
    â””â”€â”€ results/               # Trained models (gitignored)
```

## Usage Examples

### RoBERT (Recommended)

```python
from models.robert_model import RoBERTModel

# Load trained RoBERT model
model = RoBERTModel.load('results/experiments/robert_experiment/robert_model')

# Predict sentiment
review = "Produsul este excelent! Foarte mulÈ›umit."
prediction, probs = model.predict(review, return_probs=True)

sentiment = 'Positive' if prediction == 1 else 'Negative'
confidence = probs[prediction]

print(f"Sentiment: {sentiment}")
print(f"Confidence: {confidence:.2%}")
# Output: Sentiment: Positive, Confidence: 97.35%
```

### XGBoost (Fast Alternative)

```python
from pathlib import Path
from models.xgboost_model import XGBoostModel
from features.selector import FeatureSelector
from preprocessing.pipeline import PreprocessingPipeline

# Load trained XGBoost model
exp_dir = Path('results/experiments/xgb_experiment')
preprocessor = PreprocessingPipeline.load(exp_dir / 'preprocessor.joblib')
selector = FeatureSelector.load(exp_dir / 'feature_selector.joblib')
model = XGBoostModel.load(exp_dir / 'xgboost_model.joblib')

# Predict sentiment
review = "Produsul este excelent! Foarte mulÈ›umit."
processed = preprocessor.preprocess_batch([review])
features = selector.transform(processed)
prediction = model.predict(features)
confidence = model.predict_proba(features)[0][prediction[0]]

print(f"Sentiment: {'Positive' if prediction[0] == 1 else 'Negative'}")
print(f"Confidence: {confidence:.2%}")
```

## Performance Comparison

| Metric | RoBERT | XGBoost | Winner |
|--------|--------|---------|--------|
| **Accuracy** | **94-95%** | 91.6% | ðŸ† RoBERT |
| **Precision** | **94-95%** | 91.6% | ðŸ† RoBERT |
| **Recall** | **94-95%** | 91.6% | ðŸ† RoBERT |
| **F1 Score** | **94-95%** | 91.6% | ðŸ† RoBERT |
| **Training Time** | 1-3 hours | 7-60 min | ðŸ† XGBoost |
| **Inference Speed** | ~20ms | <1ms | ðŸ† XGBoost |
| **Model Size** | ~500MB | ~10MB | ðŸ† XGBoost |
| **GPU Required** | Recommended | No | ðŸ† XGBoost |

### When to Use Each Model

**Use RoBERT when:**
- Maximum accuracy is critical
- GPU is available
- Inference latency <100ms is acceptable
- Romanian language nuances are important

**Use XGBoost when:**
- Fast inference is critical (<1ms)
- No GPU available
- Small model size is required
- Resource-constrained deployment

## Requirements

- Python 3.8+
- 8 GB RAM (16 GB recommended)
- Optional: NVIDIA GPU for faster preprocessing

## License

MIT License

## Citation

```bibtex
@software{recenz_ia,
  title={Recenz-IA: Romanian Sentiment Analysis for e-commerce},
  author={Tudor Tuns},
  year={2025},
  url={https://github.com/TunsTudor-Mircea/recenz-ia}
}
```
